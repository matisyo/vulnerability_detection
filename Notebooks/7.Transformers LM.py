from pathlib import Path
from tokenizers.implementations import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
from transformers import RobertaTokenizerFast
from transformers import RobertaForMaskedLM
from transformers import DataCollatorForLanguageModeling

from transformers import LineByLineTextDataset
#https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py
#Deprecado! Se deberia hacer diferente

import pandas as pd

DATASETS_PATH = '../data/datasets'
TOKENIZERS_PATH = '../data/tokenizers'
LANG_MODEL_DATASET = f'{DATASETS_PATH}/lang_model_dataset.csv'
OPCODES_TXT = f'{DATASETS_PATH}/full_opcodes.txt'
TOKENIZER_PATH = f'{TOKENIZERS_PATH}/tokenizer_opcode_bpe'
MODEL_PATH = f'../data/models/roberta_lm'

dataset = pd.read_csv(LANG_MODEL_DATASET)
dataset['n_size'] = dataset.opcode_nocds.apply(lambda x:len(x.split(" ")))
with open(OPCODES_TXT,'w') as f:
    for o,v,k in dataset[dataset.n_size<256].values:
        f.write(o+"\n")

tokenizer = ByteLevelBPETokenizer()
tokenizer.train(files=[OPCODES_TXT], vocab_size=1000, min_frequency=2, special_tokens=[
    "<s>",
    "<pad>",
    "</s>",
    "<unk>",
    "<mask>",
])
tokenizer.save_model(TOKENIZER_PATH)

tokenizer = ByteLevelBPETokenizer(f'{TOKENIZER_PATH}/vocab.json',f'{TOKENIZER_PATH}/merges.txt')

from transformers import RobertaConfig

config = RobertaConfig(
    vocab_size=1000,
    max_position_embeddings=514,
    hidden_size=216,
    num_attention_heads=6,
    num_hidden_layers=4,
    type_vocab_size=1

    # num_attention_heads=12,
    # num_hidden_layers=6,
    # type_vocab_size=1,
)


tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)
model = RobertaForMaskedLM(config=config)

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=OPCODES_TXT,
    block_size=128,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=MODEL_PATH,
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_gpu_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()
trainer.save_model(MODEL_PATH)
